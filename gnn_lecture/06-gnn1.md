# 복습

그래프를 기계학습의 소스로 이용하기 위해서는 인코딩과 디코딩 과정을 거쳐야 합니다. 인코딩은 노드 임베딩으로, 노드들을 특징 컬럼으로 형 변환시키는 것입니다. 기계학습의 소스로 이용하기 위해서는 행렬 형태로 표현되어야 하기 때문이죠. 디코딩은 인코딩된 임베딩들에 유사도를 계산하는 작업입니다. 계산된 유사도는 실제 네트워크의 노드 간 유사도를 근사하는데 쓰입니다. 노드 사이의 관계를 기계학습으로 계산할 수 있게 하는 것이 목표이죠. 

deep encoder를 살펴보기 전에 이전에 학습했던 shallow encoder의 한계를 먼저 짚고 가겠습니다. shallow encoder는 노드들의 모든 특징들을 원소로 하는 컬럼을 생성합니다. 따라서 d 차원으로 임베딩할 경우 $O(|V|)$의 거대한 매개변수들이 생성되어 문제가 됩니다. 과잉적합의 원인이 될 수 있죠. deep encoder를 거쳐 생성된 임베딩들은 shallow encoder를 거쳐 생성된 임베딩보다 당연히 좋은 성능을 기대할 수 있습니다. 

<br>

# 딥러닝의 기초

딥러닝은 현실의 문제를 최적화 문제로 변환합니다. 손실함수의 결과, 즉 손실을 가장 줄이는 방향으로 함수를 설계하는 것을 의미합니다. CrossEntropy라는 손실함수가 일반적으로 사용됩니다. 

학습은 손실 값을 역전파하고 손실 값에 대한 가중치들의 그레디언트를 계산, 경사 하강법을 이용해 가중치들을 조정합니다. 

<br>

# 그래프에 대한 딥러닝

아주 간단한 방법으로 그래프를 기계학습의 소스로 사용해보겠습니다. shallow encoder를 이용해 그래프의 노드마다 m개의 원소를 갖는 임베딩을 생성합니다. 역시 간단한 디코더를 거쳤기에 노드 사이의 연결 여부가 유사도를 결정하도록 했습니다. 인접행렬에 특징 벡터를 추가한 행렬을 생성하고 이 행렬을 모델에 전달합니다. 

원리를 정확히 이해하지 못했기 때문에 다른 사이즈의 그래프에 적용할 수 없는 이유와 순서에 민감한 이유를 설명할 수 없습니다. 

<br>

# gcn

## 이웃 노드들의 종합 방식

이전의 간단한 방법에서 발생한 문제 중 하나는 가중치의 개수가 아주 많아진다는 것이었습니다. 많은 가중치는 과잉적합 문제를 발생시킵니다. 단순한 자료형태를 소스로 하는 신경망에서도 이 문제를 해결하기 위해 컨벌루션 기법을 적용했습니다. 그래프를 이용한 기계학습도 이 기법을 이용했습니다. 타겟팅하는 노드의 임베딩을 계산하기 위해 주변 노드들의 메시지를 종합하는 것입니다. 이 과정이 deep encoder 입니다. 

deep encoder 역시 하나의 모델입니다. 이 모델은 임의의 깊이를 가질 수 있습니다. 노드들은 각 레이어마다 다른 임베딩을 갖습니다. 0번째 레이어의 경우는 노드의 특징 벡터 자체이고 k번째 레이어의 경우는 k번의 홉을 거친 종합 메시지에 영향을 받은 임베딩이 만들어집니다. 이때 종합되는 노드들의 순서에 영향을 받아서는 안되며 이 개념이 permutation invariant 입니다. 

이웃 노드들을 종합하는 기본적인 접근방법은 이웃 노드들의 메시지를 종합해서 평균을 내는 것입니다. 이 방법은 이웃 노드들의 순서에 무관하며 항상 동일한 값을 반환합니다. 

이 방식을 수식으로 표현하면 k+1번째 레이어의 노드 v의 임베딩을 다음과 같이 표현할 수 있습니다. 

$h_v^{k+1} = {
\sigma ({
W_k \Sigma_{u \in N(v)} {h_u^{k} \over |N(v)|}
\ \ + \ \ B_k h_v^k 
)
}
}$  

k번째 레이어에서 v의 이웃한 노드들로부터 전달받는 메시지의 평균의 가중합을 계산하고, 이에 역시 k번째 레이어의 v 노드, 자기 자신의 노드에 대한 가중합을 계산하여 더한 뒤 활성화 함수를 적용합니다. 

위 과정이 L 개의 레이어에 거쳐 이루어지면 결과적으로 노드 v의 임베딩, $z_v$가 생성됩니다.

## 모델 훈련

기계학습에서의 훈련은 역전파 원리를 이용해 가중치를 조정하는 것입니다. 딥 인코더에서 조정될 가중치는 $W_k$와 $B_k$입니다. 모든 노드들은 k+1번째 레이어의 임베딩을 계산하기 위해 k번째 레이어의 자기 자신 노드의 가중합을 계산합니다. 이때 모든 노드들이 동일한 변환 가중치 행렬을 사용한다는 점이 중요하다고 언급되었습니다만, 왜 중요한지는 이해하지 못했습니다. 

행렬로 수식을 변환하는 것이 더 간단합니다. 이웃 노드들의 메시지를 종합하는 것을 행렬로 표현하면 다음과 같습니다. 

$H^{k+1} = D^{-1}AH^k$

k+1번째 레이어의 노드들의 임베딩들을 종합한 행렬은, 이전 노드 임베딩 행렬에 인접행렬, 대각행렬의 역행렬을 곱한 값으로 표현할 수 있습니다. 행렬 개념이 많이 부족합니다. 

학습은 지도학습과 비지도학습으로 나뉘어집니다. 지도학습의 경우 정답 라벨이 존재합니다. 비지도학습의 경우 크로스엔트로피의 반환값들의 종합을 loss로 설정, loss를 최소화하는 방향으로 가중치들을 학습시킵니다. 이때 크로스엔티로피 함수의 인자로 사용되는 것들은 두 노드의 유사성과 두 노드 임베딩의 디코딩 값입니다. 

이때 두 노드 u, v의 유사성을 계산하기 위해서 Random Walks(Node2Vec) 알고리즘이 사용될 수 있습니다. 그리고 디코딩은 두 임베딩의 내적값을 계산합니다. 

지도학습은 노드 분류 문제를 해결하는 것으로 예시를 들 수 있습니다. 인코더를 거쳐 생성된 임베딩에 활성화 함수를 적용하고, 이 값과 정답 레이블을 인자로 하는 손실 함수의 값을 이용해서 산출된 손실을 학습에 이용합니다. 디코더가 불필요하다고 생각됩니다. 

지도학습에서도 당연히 크로스엔트로피를 사용할 수도 있습니다. 강의에서 예로 든 것은 BCE, 이진분류 문제입니다. 아래 수식을 뜯어보도록 하겠습니다.

$Loss = \Sigma _{v \in V} {
\ \ (y_v log(\sigma (z_v^T \theta))
\ \ + \ \ (1-y_v) log(1 - \sigma (z_v^T \theta) 
)
}$

$y_v$는 정답 레이블로 0 또는 1의 값만을 갖습니다. 예를 들어 정답 레이블의 값이 1이라면 + 이하의 계산식은 0으로 사라지게 됩니다. + 이전의 값들에 대해서만 누적되게 됩니다. $\theta$는 출력층에서 사용되는 가중치 행렬입니다. 인코더를 거쳐 생성된 임베딩에 적용됩니다.