# 단일 층으로 구성된 GNN

## 메시지 계산과 종합

**gnn의 각 레이어**는 message computation(메시지 계산)과 message aggregation(메시지 종합)의 두 스텝으로 구성됩니다. 메시지는 각 노드가 부모 노드, 타겟팅 하는 노드에게 전달하는 특징 벡터입니다. 각 노드의 특징 벡터는 선형 층을 거치며 가중치 행렬에 의해 메시지로 다음 층에 전달됩니다. 메시지 종합은 타겟팅 노드가 전달받은 메시지들을 종합하는 과정입니다. 각 메시지들로 구성된 메시지 행렬에 종합 함수를 적용합니다. 이때 종합 함수는 Sum, Mean, Max 등을 사용할 수 있고, 모든 함수들은 전달받은 컬럼(메시지)들에 무관해야 합니다. 

## GCN과 GraphSAGE

GCN은 컨벌루션 개념을 이용한 그래프 신경망입니다. 메시지 종합 함수로 Sum을 사용하는 것이 특징입니다. 메시지는 계산 과정에서 노드의 차수로 정규화 됩니다. 

GraphSAGE는 GCN과 다르게 다양한 메시지 종합 함수를 사용할 수 있습니다. 그리고 GCN과 다른 점은, 이웃 노드들의 메시지를 종합한 뒤, 본인 노드의 직전 임베딩을 추가한 행렬을 만든다는 점입니다. 또한, l2 정규화 과정을 추가적으로 하기도 합니다. 각 레이어마다 생성되는 임베딩에 l2 정규화 작업을 함으로써 모든 레이어의 각 임베딩들이 l2-norm으로 정규화 됩니다. 

## GAT(Graph Attention Networks)

GCN에서 노드의 차수가 메시지 계산 과정에서 사용되었습니다. 이는 어텐션 가중치로 간선의 중요도를 의미합니다. 간선은 메시지가 전달되는 길이기 때문에 메시지의 중요도로도 해석할 수 있습니다. 따라서 GCN 같은 경우, 모든 메시지들에 동일한 중요도(어텐션)이 할당됩니다.

하지만 GAT는 각 메시지마다 다른 중요도를 할당합니다. 이 원리가 좋은 성능을 보장하는 이유는 직관적으로 이해할 수 있습니다. 바로 선택과 집중입니다. 

어텐션 메커니즘은 어텐션 계수를 임베딩 계산에 이용하는 것입니다. 이때 산출되는 어텐션 계수는 소프트맥스 함수를 거쳐 정규화 됩니다. 정규화된 어텐션 계수를 최종 어텐션 가중치라고 하며 스칼라 값입니다. 타겟팅하는 노드의 임베딩을 계산할 때, 이웃한 각 노드의 메시지들은 어텐션 계수로 조정된 뒤 종합됩니다.

어텐션 메커니즘 덕분에 생기는 장점으로 계산과 공간의 효율성이 있습니다만 이해하지 못했습니다. 지역성은 간선에 중요도가 부여되기 때문에 임베딩에 실질적으로 영향을 주는 실질 효용 범위를 제한할 수 있다는 점으로 설명될 수 있습니다. 

<br>

# 딥러닝 모듈을 활용한 GNN

gnn에 딥러닝 모듈(배치 정규화, 드롭아웃, 어텐션/게이트)들을 사용함으로써 성능을 향상시킬 수 있습니다. 

배치 정규화와 드롭아웃만 짚고 넘어가겠습니다. 배치 정규화는 신경망의 학습을 안정화시켜 줍니다. 정규화는 특정 노드(정보)에 치우치지 않고 모든 노드들에 동일한 중요성을 할애하는 것을 목적으로 합니다. 따라서 N개의 임베딩들의 평균과 분산을 계산하고, 계산한 평균과 분산으로 각 특징(임베딩)들을 정규화시킵니다. 

드롭아웃은 과잉적합을 해소하기 위해 고안된 방법입니다. gnn 같은 경우 선형 층을 통해 메시지들이 전달됩니다. 이때 학습되는 노드의 수를 임의로 감소시킴으로써 훈련 데이터에 치우쳐 학습되는 경향을 방지해줍니다. 

<br>

# GNN의 복수 층 적재

## Over-smoothing 문제

gnn은 cnn과 다르게 많은 층을 적재하는 것이 좋은 성능을 보장하지 않습니다. 그 이유는 over-smoothing(지나친 획일화) 문제 때문입니다. 지나친 획일화 문제는 노드의 임베딩들이 수렴하는 현상으로 그 원인은 receptive field(수용 영역)가 층의 개수가 증가할 때 매우 큰 폭으로 증가하기 때문입니다.

gnn에서 층을 추가함이란 hops을 증가시키는 것과 같습니다. 타겟노드의 임베딩 결정에 영향을 주는 노드들의 영역이 수용 영역입니다. 수용 영역은 hop이 증가함에 따라 매우 큰 폭으로 증가하므로 hop 수를 3으로 가정하더라도 수용 영역이 네트워크의 대부분을 포함하게 됩니다. (강의자료 기준)

## GNN 층 설계

따라서 gnn을 설계할 때는 층을 주의해서 적재해야 합니다. 문제에 알맞은 수준의 층을 적재하는 것이 첫 번째입니다. 이때, 적은 층으로 높은 표현력을 갖기 위해 사용되는 방법이 두 가지 있습니다. 첫 번째로는 각 gnn 자체의 표현력을 높이는 것입니다. 이는 각 gnn의 층을 단순 선형 층으로 설계하는 것이 아니라 MLP, DNN과 같이 더 복잡한 층으로 설계하는 것을 의미합니다. 또 다른 방법으로는 gnn 층들의 전, 후에 전처리, 후처리 층을 삽입하는 것입니다. 

하지만 해결해야하는 문제가 적은 gnn 층으로 불가능하다면, skip conncection 기법을 사용할 수 있습니다. gnn에서 많은 층을 사용하는 것은 지나친 획일화 문제를 야기합니다. 이 문제가 발생하기 전에, 즉 적은 홉으로부터 계산된 임베딩을 최종 임베딩 계산으로 넘기는 것입니다. 중간 단계를 스킵하는 것과 같은 말입니다. 이 기법으로 얕은 gnn과 깊은 gnn의 혼합물을 얻을 수 있습니다. skip connection의 이해도 부족하므로 여기서 정리를 마무리하겠습니다.
